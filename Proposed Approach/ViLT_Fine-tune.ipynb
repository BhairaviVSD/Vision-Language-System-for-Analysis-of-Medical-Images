{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcc6bed5-089b-42cd-b248-ed30d3f530c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import ViltForQuestionAnswering, ViltProcessor, ViltConfig, AdamW\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c38e2d5d-d07c-4280-b8f2-1de563aa613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ViltConfig.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e08a9e43-4d0f-4217-977e-30d7a866a34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'split': 'train', 'idx': 0, 'subject_id': '17945608', 'study_id': '55914880', 'image_id': 'e557790f-48a1ede1-2d7b2605-5f6c34c1-6713a5c0', 'image_path': 'p17/p17945608/s55914880/e557790f-48a1ede1-2d7b2605-5f6c34c1-6713a5c0.jpg', 'question': 'Is there any occurrence of anatomical findings in the left hilar structures?', 'semantic_type': 'verify', 'content_type': 'presence', 'template': 'Is there any occurrence of ${category} in the ${object}?', 'template_program': 'program_1', 'template_arguments': {'object': {'0': 'left hilar structures'}, 'attribute': {}, 'category': {'0': 'anatomicalfinding'}, 'viewpos': {}, 'gender': {}}, 'answer': ['yes']}\n"
     ]
    }
   ],
   "source": [
    "f = open('/scratch/bvs9764/physionet.org/files/mimic-ext-mimic-cxr-vqa/1.0.0/MIMIC-Ext-MIMIC-CXR-VQA/dataset/train.json')\n",
    "\n",
    "# Return JSON object as dictionary\n",
    "data_questions = json.load(f)\n",
    "print(data_questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0892b53-8af5-4508-a0b0-cd9220497404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the JSON file\n",
    "with open('/scratch/bvs9764/physionet.org/files/mimic-ext-mimic-cxr-vqa/1.0.0/MIMIC-Ext-MIMIC-CXR-VQA/dataset/train.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract relevant fields\n",
    "processed_data = []\n",
    "for record in data:\n",
    "    processed_data.append({\n",
    "        'image_path': record['image_path'],\n",
    "        'question': record['question'],\n",
    "        'answer': 1 if 'yes' in record['answer'] else 0  # Convert 'yes'/'no' to 1/0\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame and save as CSV\n",
    "df = pd.DataFrame(processed_data)\n",
    "#df.to_csv('train_processed.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d27fedf9-bf6a-4304-a03f-eed2124c4c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290031"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3654cd6e-60f0-4f30-aa12-98995de6840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "xray_transforms = transforms.Compose([\n",
    "    # Resize to match ViLT model input size\n",
    "    transforms.Resize((384, 384)),\n",
    "    \n",
    "    # Center crop to maintain focus on central anatomy\n",
    "    transforms.CenterCrop(384),\n",
    "    \n",
    "    # Random horizontal flip for data augmentation (50% chance)\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    \n",
    "    # Random rotation within a small range (+/- 10 degrees)\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    \n",
    "    # Adjust contrast to enhance visibility of structures\n",
    "    transforms.ColorJitter(contrast=0.2),\n",
    "    \n",
    "    # Convert image to tensor and normalize\n",
    "    transforms.ToTensor(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc7e471e-5d69-4799-9676-57cd0f5d73a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from transformers import ViltProcessor\n",
    "from PIL import Image\n",
    "\n",
    "class MIMICCXRQA_Dataset(Dataset):\n",
    "    def __init__(self, csv_path, data_dir, processor, transform=None):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.data_dir = data_dir\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data.iloc[idx]['image_path']\n",
    "        question = self.data.iloc[idx]['question']\n",
    "        label = self.data.iloc[idx]['answer']  # Convert the string to 0 or 1\n",
    "\n",
    "        # Load image\n",
    "        full_img_path = os.path.join(self.data_dir, img_path)\n",
    "\n",
    "        try:\n",
    "                # Load image\n",
    "            image = Image.open(full_img_path).convert(\"RGB\")\n",
    "\n",
    "                # Apply image transformations if any\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "                # Process the image-question pair\n",
    "            encoding = self.processor(\n",
    "                    images=image,\n",
    "                    text=question,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True\n",
    "                )\n",
    "\n",
    "                # Ensure the tensors are squeezed for batch loading\n",
    "            encoding = {k: v.squeeze() for k, v in encoding.items()}\n",
    "            encoding['labels'] = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "            return encoding\n",
    "\n",
    "        except (FileNotFoundError, UnidentifiedImageError) as e:\n",
    "                print(f\"Warning: Missing or corrupt file '{full_img_path}', skipping...\")\n",
    "                idx += 1  # Skip to the next data sample\n",
    "\n",
    "        # If all subsequent images are missing, raise an IndexError\n",
    "        raise IndexError(f\"All images after index {idx} are missing or corrupt.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6e72672-4e17-462f-9f04-fbd0df94879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the data directory where images are stored\n",
    "data_dir = '/scratch/bvs9764/physionet.org/files/mimic-cxr-jpg/2.1.0/files'\n",
    "\n",
    "processor = ViltProcessor.from_pretrained('dandelin/vilt-b32-mlm',do_rescale = False)\n",
    "# Initialize dataset\n",
    "train_dataset = MIMICCXRQA_Dataset(\n",
    "    csv_path='/scratch/bvs9764/train_processed.csv',\n",
    "    data_dir=data_dir,\n",
    "    processor=processor,\n",
    "    transform=xray_transforms\n",
    ")\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6083e2b-2cb7-487c-b11c-ba01bfa4c126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1514b8a26290>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dbfd643-19df-439b-80f4-d03af1afa6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViltForQuestionAnswering were not initialized from the model checkpoint at dandelin/vilt-b32-mlm and are newly initialized: ['classifier.0.bias', 'classifier.0.weight', 'classifier.1.bias', 'classifier.1.weight', 'classifier.3.bias', 'classifier.3.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViltForQuestionAnswering(\n",
       "  (vilt): ViltModel(\n",
       "    (embeddings): ViltEmbeddings(\n",
       "      (text_embeddings): TextEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768)\n",
       "        (position_embeddings): Embedding(40, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (patch_embeddings): ViltPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
       "      )\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViltEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViltLayer(\n",
       "          (attention): ViltAttention(\n",
       "            (attention): ViltSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViltSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViltIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViltOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViltPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=1536, bias=True)\n",
       "    (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Linear(in_features=1536, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-mlm\",num_labels = 2)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bef9794-1209-4b14-8d35-7db50b576ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bvs9764/.local/lib/python3.11/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1:   2%|‚ñè         | 1440/72508 [14:07<11:50:23,  1.67it/s, loss=0.437]"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Freeze ViLT transformer layers to speed up fine-tuning\n",
    "for param in model.vilt.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Ensure the classification head is trainable\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        try:\n",
    "            # Move inputs to the device\n",
    "            for k, v in batch.items():\n",
    "                batch[k] = v.to(device)\n",
    "\n",
    "            # Expand labels to match the logits shape\n",
    "            labels = batch['labels']  # Assuming labels are in the batch\n",
    "            batch_size = labels.shape[0]\n",
    "            expanded_labels = torch.zeros(batch_size, 2, device=labels.device)\n",
    "            expanded_labels[torch.arange(batch_size), labels] = 1\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=batch['input_ids'],\n",
    "                            pixel_values=batch['pixel_values'],\n",
    "                            attention_mask=batch['attention_mask'],\n",
    "                            labels=expanded_labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            loop.set_description(f\"Epoch {epoch + 1}\")\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during training: {e}, skipping batch...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0491984-dfaf-4082-8051-2733c32bcfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory to save the model and processor\n",
    "save_directory = \"vilt_finetuned_vqa\"\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(save_directory)\n",
    "\n",
    "# Save the processor\n",
    "processor.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and processor saved to {save_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f771e235-6d79-4494-9622-0f37b1a1038c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
